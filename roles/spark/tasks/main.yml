- name: 解压源码包
  become: true
  shell: "tar zxf {{ WORKSPACE_HOME }}/download/spark-{{ SPARK_VERSION }}-bin-hadoop2.7.tgz -C /opt/"

- name: 设置软连接
  become: true
  file: 
    src: /opt/spark-{{ SPARK_VERSION }}-bin-hadoop2.7
    dest: "{{ SPARK_HOME }}"
    state: link
    owner: hadoop
    group: hadoop

- name: 配置权限
  become: true
  shell: "chown -R hadoop.hadoop {{ SPARK_HOME }} /opt/spark-{{ SPARK_VERSION }}-bin-hadoop2.7"

- name: 配置环境变量
  become: true
  lineinfile:
    dest: "{{ item }}"
    state: present
    line: |
      # Spark Path
      export SPARK_HOME={{ SPARK_HOME }}
      export PATH=$PATH:$SPARK_HOME/bin
  with_items:
    - /etc/profile
    - /home/hadoop/.bashrc

- name: source env
  shell: "source {{ item }}"
  with_items:
    - /etc/profile
    - /home/hadoop/.bashrc

- name: 创建配置文件
  template: 
    src: "{{ item }}.j2"
    dest: "{{ SPARK_HOME }}/conf/{{ item }}"
    owner: hadoop
    group: hadoop
  with_items:
    - spark-env.sh
    - spark-defaults.conf
    - metrics.properties 

- name: 配置workers
  lineinfile:
    dest: "{{ SPARK_HOME }}/conf/workers"
    line: "127.0.0.1"
    create: yes

- name: 配置hive
  command: "/usr/bin/cp -r {{ HIVE_HOME }}/conf/hive-site.xml {{ SPARK_HOME }}/conf/"

- name: 拷贝依赖的jar包
  copy:
    src: "{{ WORKSPACE_HOME }}/download/mysql-connector-java-5.1.49.jar"
    dest: "{{ SPARK_HOME }}/jars/"
    owner: hadoop
    group: hadoop

- name: 启动spark
  shell:
    chdir: "{{ SPARK_HOME }}"
    cmd: bash ./sbin/start-all.sh
  tags:
    - restart

- name: 检查端口是否运行
  wait_for: port=7077 state=started delay=1 timeout=30

- name: 测试spark命令
  command: spark-sql -e "show databases"
  register: spark_info

- name: 打印测试结果
  debug: var=spark_info.stdout_lines
